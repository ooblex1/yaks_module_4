https://thayer.github.io/engs50/Notes/search/activity.html
3
3515
<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Activity - Crawler</title>
  <meta name="description" content="In Engs 50 you will learn how to design & build large, reliable, maintainable, and understandable software systems.  In the process you will learn to program in C with Unix development tools.
">

  <link rel="shortcut icon" href="/engs50/50.png" />
  <link rel="stylesheet" href="/engs50/css/main.css">
  <link rel="canonical" href="/engs50/Notes/search/activity.html">
</head>


  <body>

    <header class="site-header">

    <a class="site-title" href="/engs50/">
      <img width=48 align=center src="/engs50/50.png" alt="icon">
      Engs50 (CS50)
    </a>

    <nav class="site-nav">
      [<a href="/engs50/Labs/">Labs</a>]
      [<a href="/engs50/Notes/">Notes</a>]
      [<a href="/engs50/Reading/">Reading</a>]
      [<a href="/engs50/Resources/">Resources</a>]
			[<a href="/engs50/Examples/">Examples</a>]
   </nav>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Activity - Crawler</h1>
  </header>

  <div class="post-content">
    <h3 id="goals">Goals</h3>

<ul>
  <li>to start thinking about a design for the <em>Crawler</em></li>
</ul>

<h3 id="activity">Activity</h3>

<p>In your group, consider the following informal description of the <strong>Crawler</strong>:</p>

<ul>
  <li>It takes three parameters: the URL for a web page to use as a starting point (<em>seed</em>) for the crawl, the <em>maximum depth</em> it should crawl from that seed, and the name of a <em>directory</em> where it can cache copies of the web pages it crawls.</li>
  <li>It should start from a given URL called the <em>seed</em>.
The web page at that URL is said to be at depth 0.</li>
  <li>It should <em>explore</em> that URL; that is, it should download the web page at that URL, and scan that page’s HTML for embedded links to URLs.
(Assume you are given a function that can pick URLs out of HTML.)  When exploring a page at depth <em>d</em>, its embedded URLs refer to pages that are said to be at depth <em>d+1</em>.</li>
  <li>Ignore URLs that don’t point at HTML.</li>
  <li>Ignore URLs at depth greater than <em>maxDepth</em>.</li>
  <li>Explore each non-ignored URL by downloading its HTML and scanning that HTML for URLs, as above.</li>
  <li>For each page it explores, it should create one file that contains the URL of that page, its depth in the crawl, and the HTML for that page.</li>
</ul>

<p><strong><em>Discuss how you could structure a crawler to accomplish the above goals – probably two nested loops – and leverage your Lab 3 data structures.</em></strong></p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Engs50 (CS50) -- Dartmouth College</h2>

    <p> <font size=-1>
    	This version of the course is based upon those designed by
    	Professors Palmer, Kotz, Zhou, Campbell, and Balkcom.
	I am deeply indebted to these outstanding educators.
    -- <a href="https://engineering.dartmouth.edu/people/faculty/stephen-taylor/">Stephen Taylor</a>
       </font>
    </p>
    <p><small>This page was last updated on <strong>2018-10-10</strong> at <strong>18:07</strong>.</small></p>
  </div>

</footer>


  </body>

</html>

