https://thayer.github.io/engs50/Labs/Lab4/REQUIREMENTS.html
3
5030
<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>TSE Crawler Requirements Spec</title>
  <meta name="description" content="In Engs 50 you will learn how to design & build large, reliable, maintainable, and understandable software systems.  In the process you will learn to program in C with Unix development tools.
">

  <link rel="shortcut icon" href="/engs50/50.png" />
  <link rel="stylesheet" href="/engs50/css/main.css">
  <link rel="canonical" href="/engs50/Labs/Lab4/REQUIREMENTS.html">
</head>


  <body>

    <header class="site-header">

    <a class="site-title" href="/engs50/">
      <img width=48 align=center src="/engs50/50.png" alt="icon">
      Engs50 (CS50)
    </a>

    <nav class="site-nav">
      [<a href="/engs50/Labs/">Labs</a>]
      [<a href="/engs50/Notes/">Notes</a>]
      [<a href="/engs50/Reading/">Reading</a>]
      [<a href="/engs50/Resources/">Resources</a>]
			[<a href="/engs50/Examples/">Examples</a>]
   </nav>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">TSE Crawler Requirements Spec</h1>
  </header>

  <div class="post-content">
    <p>The TSE crawler is a standalone program that crawls the web and retrieves webpages starting from a “seed” URL.
It parses the seed webpage, extracts any embedded URLs, then retrieves each of those pages, recursively, but limiting its exploration to a given “depth”.</p>

<p>The crawler <strong>shall</strong>:</p>

<ol>
  <li>execute from a command line with usage syntax
    <ul>
      <li><code class="highlighter-rouge">./crawler seedURL pageDirectory maxDepth</code></li>
      <li>where <code class="highlighter-rouge">seedURL</code> is used as the initial URL,</li>
      <li>where <code class="highlighter-rouge">pageDirectory</code> is the pathname for an existing directory in which to write downloaded webpages, and</li>
      <li>where <code class="highlighter-rouge">maxDepth</code> is a non-negative integer representing the maximum crawl depth.</li>
    </ul>
  </li>
  <li>crawl all pages reachable from <code class="highlighter-rouge">seedURL</code>, following  links to a maximum depth of <code class="highlighter-rouge">maxDepth</code>; where <code class="highlighter-rouge">maxDepth=0</code> means that crawler only explores the page at <code class="highlighter-rouge">seedURL</code>, <code class="highlighter-rouge">maxDepth=1</code> means that crawler only explores the page at <code class="highlighter-rouge">seedURL</code> and those pages to which <code class="highlighter-rouge">seedURL</code> links, and so forth inductively.</li>
  <li>pause at least one second between page fetches.</li>
  <li>ignore URLs that are not “internal” (meaning, outside the designated CS50 server).</li>
  <li>write each explored page to the <code class="highlighter-rouge">pageDirectory</code> with a unique document ID, wherein
    <ul>
      <li>the document <code class="highlighter-rouge">id</code> starts at 1 and increments by 1 for each new page,</li>
      <li>and the filename is of form <code class="highlighter-rouge">pageDirectory/id</code>,</li>
      <li>and the first line of the file is the URL,</li>
      <li>and the second line of the file is the depth,</li>
      <li>and the rest of the file is the page content (the HTML, unchanged).</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>In a requirements spec, <strong>shall do</strong> means <strong>must do</strong>.</p>
</blockquote>

<h4 id="be-polite">Be polite</h4>

<p>Webservers do not like crawlers (think about why).
Indeed, it you hit a web server too hard, its operator may block your crawler based on its Internet address.
Actually, they’ll usually block your whole domain.
A hyperactive CS50 crawler could cause some websites to block the whole of <code class="highlighter-rouge">dartmouth.edu</code>.</p>

<p>To be polite, our crawler purposely slows its behavior by introducing a delay, sleeping for one second between fetches.</p>

<p>Furthermore, our crawler will limit its crawl to a specific web <a href="http://old-www.cs.dartmouth.edu">server</a> inside CS, so we don’t bother any other servers on campus or beyond.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Engs50 (CS50) -- Dartmouth College</h2>

    <p> <font size=-1>
    	This version of the course is based upon those designed by
    	Professors Palmer, Kotz, Zhou, Campbell, and Balkcom.
	I am deeply indebted to these outstanding educators.
    -- <a href="https://engineering.dartmouth.edu/people/faculty/stephen-taylor/">Stephen Taylor</a>
       </font>
    </p>
    <p><small>This page was last updated on <strong>2018-10-10</strong> at <strong>18:07</strong>.</small></p>
  </div>

</footer>


  </body>

</html>

